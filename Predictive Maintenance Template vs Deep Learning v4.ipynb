{"nbformat_minor": 2, "cells": [{"source": "# Deep Learning for Predictive Maintenance\n\nDeep learning has proven to show superior performance in certain domains such as object recognition and image classification. It is has also gained popularity in  domains such as finance where time-series data plays an important role. Predictive Maintenance is also a domain where data is collected over time to monitor the state of an asset with the goal of finding patterns to predict failures which can also benefit from certain deep learning algorithms. Among the deep learning methods, Long Short Time Memory [(LSTM)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) networks are especially appealing to the predictive maintenance domain due to the fact that they are very good at learning from sequences. This fact lends itself to their applications using time series data by making it possible to look back for longer periods of time to detect failure patterns. In this notebook, we build an LSTM network for the data set and scenerio described at [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) to predict remaining useful life of aircraft engines. In summary, the template uses simulated aircraft sensor values to predict when an aircraft engine will fail in the future so that maintenance can be planned in advance. \n\nThis notebook serves as a tutorial for beginners looking to apply deep learning in predictive maintenance domain and uses a simple scenario where only one data source (sensor values) is used to make predictions. In more advanced predictive maintenance scenarios such as in [Predictive Maintenance Modelling Guide](https://gallery.cortanaintelligence.com/Notebook/Predictive-Maintenance-Modelling-Guide-R-Notebook-1), there are many other data sources such as maintenance records, error logs etc. which may require different types of treatments to be used in the deep learning networks. Since predictive maintenance is not a typical domain for deep learning, its application is an open area of research. \n\nThis notebook uses [keras](https://keras.io/) deep learning library.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 1, "cell_type": "code", "source": "#!pip install keras\nimport pandas as pd\nimport numpy as np\nnp.random.seed(12345)  # setting seed for reproducability\nPYTHONHASHSEED = 0\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Activation\n%matplotlib inline", "outputs": [{"output_type": "stream", "name": "stderr", "text": "Using TensorFlow backend.\n"}], "metadata": {"collapsed": false}}, {"source": "## Data Ingestion\nIn the following, we ingest the training, test and ground truth datasets from azure storage.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 3, "cell_type": "code", "source": "!wget http://azuremlsamples.azureml.net/templatedata/PM_train.txt \n!wget http://azuremlsamples.azureml.net/templatedata/PM_test.txt\n!wget http://azuremlsamples.azureml.net/templatedata/PM_truth.txt ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "--2017-06-02 17:42:40--  http://azuremlsamples.azureml.net/templatedata/PM_train.txt\nResolving webproxy (webproxy)... 100.116.234.101\nConnecting to webproxy (webproxy)|100.116.234.101|:3128... connected.\nProxy request sent, awaiting response... 200 OK\nLength: 3515356 (3.4M) [text/plain]\nSaving to: 'PM_train.txt.5'\n\nPM_train.txt.5      100%[===================>]   3.35M  22.3MB/s    in 0.2s    \n\n2017-06-02 17:42:41 (22.3 MB/s) - 'PM_train.txt.5' saved [3515356/3515356]\n\n--2017-06-02 17:42:41--  http://azuremlsamples.azureml.net/templatedata/PM_test.txt\nResolving webproxy (webproxy)... 100.116.234.101\nConnecting to webproxy (webproxy)|100.116.234.101|:3128... connected.\nProxy request sent, awaiting response... 200 OK\nLength: 2228855 (2.1M) [text/plain]\nSaving to: 'PM_test.txt.4'\n\nPM_test.txt.4       100%[===================>]   2.12M  --.-KB/s    in 0.02s   \n\n2017-06-02 17:42:41 (91.2 MB/s) - 'PM_test.txt.4' saved [2228855/2228855]\n\n--2017-06-02 17:42:41--  http://azuremlsamples.azureml.net/templatedata/PM_truth.txt\nResolving webproxy (webproxy)... 100.116.234.101\nConnecting to webproxy (webproxy)|100.116.234.101|:3128... connected.\nProxy request sent, awaiting response... 200 OK\nLength: 429 [text/plain]\nSaving to: 'PM_truth.txt.5'\n\nPM_truth.txt.5      100%[===================>]     429  --.-KB/s    in 0s      \n\n2017-06-02 17:42:41 (83.5 MB/s) - 'PM_truth.txt.5' saved [429/429]\n\n"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "# read training data \ntrain_df = pd.read_csv('PM_train.txt', sep=\" \", header=None)\ntrain_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\ntrain_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n                     's15', 's16', 's17', 's18', 's19', 's20', 's21']", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 9, "cell_type": "code", "source": "# read test data\ntest_df = pd.read_csv('PM_test.txt', sep=\" \", header=None)\ntest_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\ntest_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n                     's15', 's16', 's17', 's18', 's19', 's20', 's21']", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 10, "cell_type": "code", "source": "# read ground truth data\ntruth_df = pd.read_csv('PM_truth.txt', sep=\" \", header=None)\ntruth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 11, "cell_type": "code", "source": "train_df = train_df.sort_values(['id','cycle'])\ntrain_df.head()", "outputs": [{"execution_count": 11, "output_type": "execute_result", "data": {"text/plain": "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n\n      s5   ...        s12      s13      s14     s15   s16  s17   s18    s19  \\\n0  14.62   ...     521.66  2388.02  8138.62  8.4195  0.03  392  2388  100.0   \n1  14.62   ...     522.28  2388.07  8131.49  8.4318  0.03  392  2388  100.0   \n2  14.62   ...     522.42  2388.03  8133.23  8.4178  0.03  390  2388  100.0   \n3  14.62   ...     522.86  2388.08  8133.83  8.3682  0.03  392  2388  100.0   \n4  14.62   ...     522.19  2388.04  8133.80  8.4294  0.03  393  2388  100.0   \n\n     s20      s21  \n0  39.06  23.4190  \n1  39.00  23.4236  \n2  38.95  23.3442  \n3  38.88  23.3739  \n4  38.90  23.4044  \n\n[5 rows x 26 columns]", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>id<\/th>\n      <th>cycle<\/th>\n      <th>setting1<\/th>\n      <th>setting2<\/th>\n      <th>setting3<\/th>\n      <th>s1<\/th>\n      <th>s2<\/th>\n      <th>s3<\/th>\n      <th>s4<\/th>\n      <th>s5<\/th>\n      <th>...<\/th>\n      <th>s12<\/th>\n      <th>s13<\/th>\n      <th>s14<\/th>\n      <th>s15<\/th>\n      <th>s16<\/th>\n      <th>s17<\/th>\n      <th>s18<\/th>\n      <th>s19<\/th>\n      <th>s20<\/th>\n      <th>s21<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>1<\/td>\n      <td>1<\/td>\n      <td>-0.0007<\/td>\n      <td>-0.0004<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>641.82<\/td>\n      <td>1589.70<\/td>\n      <td>1400.60<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>521.66<\/td>\n      <td>2388.02<\/td>\n      <td>8138.62<\/td>\n      <td>8.4195<\/td>\n      <td>0.03<\/td>\n      <td>392<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>39.06<\/td>\n      <td>23.4190<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>1<\/td>\n      <td>2<\/td>\n      <td>0.0019<\/td>\n      <td>-0.0003<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.15<\/td>\n      <td>1591.82<\/td>\n      <td>1403.14<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>522.28<\/td>\n      <td>2388.07<\/td>\n      <td>8131.49<\/td>\n      <td>8.4318<\/td>\n      <td>0.03<\/td>\n      <td>392<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>39.00<\/td>\n      <td>23.4236<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>1<\/td>\n      <td>3<\/td>\n      <td>-0.0043<\/td>\n      <td>0.0003<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.35<\/td>\n      <td>1587.99<\/td>\n      <td>1404.20<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>522.42<\/td>\n      <td>2388.03<\/td>\n      <td>8133.23<\/td>\n      <td>8.4178<\/td>\n      <td>0.03<\/td>\n      <td>390<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>38.95<\/td>\n      <td>23.3442<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>1<\/td>\n      <td>4<\/td>\n      <td>0.0007<\/td>\n      <td>0.0000<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.35<\/td>\n      <td>1582.79<\/td>\n      <td>1401.87<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>522.86<\/td>\n      <td>2388.08<\/td>\n      <td>8133.83<\/td>\n      <td>8.3682<\/td>\n      <td>0.03<\/td>\n      <td>392<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>38.88<\/td>\n      <td>23.3739<\/td>\n    <\/tr>\n    <tr>\n      <th>4<\/th>\n      <td>1<\/td>\n      <td>5<\/td>\n      <td>-0.0019<\/td>\n      <td>-0.0002<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.37<\/td>\n      <td>1582.85<\/td>\n      <td>1406.22<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>522.19<\/td>\n      <td>2388.04<\/td>\n      <td>8133.80<\/td>\n      <td>8.4294<\/td>\n      <td>0.03<\/td>\n      <td>393<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>38.90<\/td>\n      <td>23.4044<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n<p>5 rows \u00c3\u0097 26 columns<\/p>\n<\/div>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "## Data Preprocessing\nFirst step is to generate labels for the training data which are Remaining Useful Life (RUL), label1 and label2 as was done in the [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3). Here, we will only make use of \"label1\" for binary clasification.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "# generate column RUL\nrul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\nrul.columns = ['id', 'max']\ntrain_df = train_df.merge(rul, on=['id'], how='left')\ntrain_df['RUL'] = train_df['max'] - train_df['cycle']\ntrain_df.drop('max', axis=1, inplace=True)\ntrain_df.head()", "outputs": [{"execution_count": 12, "output_type": "execute_result", "data": {"text/plain": "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n\n      s5 ...       s13      s14     s15   s16  s17   s18    s19    s20  \\\n0  14.62 ...   2388.02  8138.62  8.4195  0.03  392  2388  100.0  39.06   \n1  14.62 ...   2388.07  8131.49  8.4318  0.03  392  2388  100.0  39.00   \n2  14.62 ...   2388.03  8133.23  8.4178  0.03  390  2388  100.0  38.95   \n3  14.62 ...   2388.08  8133.83  8.3682  0.03  392  2388  100.0  38.88   \n4  14.62 ...   2388.04  8133.80  8.4294  0.03  393  2388  100.0  38.90   \n\n       s21  RUL  \n0  23.4190  191  \n1  23.4236  190  \n2  23.3442  189  \n3  23.3739  188  \n4  23.4044  187  \n\n[5 rows x 27 columns]", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>id<\/th>\n      <th>cycle<\/th>\n      <th>setting1<\/th>\n      <th>setting2<\/th>\n      <th>setting3<\/th>\n      <th>s1<\/th>\n      <th>s2<\/th>\n      <th>s3<\/th>\n      <th>s4<\/th>\n      <th>s5<\/th>\n      <th>...<\/th>\n      <th>s13<\/th>\n      <th>s14<\/th>\n      <th>s15<\/th>\n      <th>s16<\/th>\n      <th>s17<\/th>\n      <th>s18<\/th>\n      <th>s19<\/th>\n      <th>s20<\/th>\n      <th>s21<\/th>\n      <th>RUL<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>1<\/td>\n      <td>1<\/td>\n      <td>-0.0007<\/td>\n      <td>-0.0004<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>641.82<\/td>\n      <td>1589.70<\/td>\n      <td>1400.60<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>2388.02<\/td>\n      <td>8138.62<\/td>\n      <td>8.4195<\/td>\n      <td>0.03<\/td>\n      <td>392<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>39.06<\/td>\n      <td>23.4190<\/td>\n      <td>191<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>1<\/td>\n      <td>2<\/td>\n      <td>0.0019<\/td>\n      <td>-0.0003<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.15<\/td>\n      <td>1591.82<\/td>\n      <td>1403.14<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>2388.07<\/td>\n      <td>8131.49<\/td>\n      <td>8.4318<\/td>\n      <td>0.03<\/td>\n      <td>392<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>39.00<\/td>\n      <td>23.4236<\/td>\n      <td>190<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>1<\/td>\n      <td>3<\/td>\n      <td>-0.0043<\/td>\n      <td>0.0003<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.35<\/td>\n      <td>1587.99<\/td>\n      <td>1404.20<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>2388.03<\/td>\n      <td>8133.23<\/td>\n      <td>8.4178<\/td>\n      <td>0.03<\/td>\n      <td>390<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>38.95<\/td>\n      <td>23.3442<\/td>\n      <td>189<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>1<\/td>\n      <td>4<\/td>\n      <td>0.0007<\/td>\n      <td>0.0000<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.35<\/td>\n      <td>1582.79<\/td>\n      <td>1401.87<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>2388.08<\/td>\n      <td>8133.83<\/td>\n      <td>8.3682<\/td>\n      <td>0.03<\/td>\n      <td>392<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>38.88<\/td>\n      <td>23.3739<\/td>\n      <td>188<\/td>\n    <\/tr>\n    <tr>\n      <th>4<\/th>\n      <td>1<\/td>\n      <td>5<\/td>\n      <td>-0.0019<\/td>\n      <td>-0.0002<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.37<\/td>\n      <td>1582.85<\/td>\n      <td>1406.22<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>2388.04<\/td>\n      <td>8133.80<\/td>\n      <td>8.4294<\/td>\n      <td>0.03<\/td>\n      <td>393<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>38.90<\/td>\n      <td>23.4044<\/td>\n      <td>187<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n<p>5 rows \u00c3\u0097 27 columns<\/p>\n<\/div>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "# generate label columns  for training data\nw1 = 30\nw0 = 15\ntrain_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\ntrain_df['label2'] = train_df['label1']\ntrain_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\ntrain_df.head()", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n3   1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79  1401.87   \n4   1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85  1406.22   \n\n      s5   ...       s15   s16  s17   s18    s19    s20      s21  RUL  label1  \\\n0  14.62   ...    8.4195  0.03  392  2388  100.0  39.06  23.4190  191       0   \n1  14.62   ...    8.4318  0.03  392  2388  100.0  39.00  23.4236  190       0   \n2  14.62   ...    8.4178  0.03  390  2388  100.0  38.95  23.3442  189       0   \n3  14.62   ...    8.3682  0.03  392  2388  100.0  38.88  23.3739  188       0   \n4  14.62   ...    8.4294  0.03  393  2388  100.0  38.90  23.4044  187       0   \n\n   label2  \n0       0  \n1       0  \n2       0  \n3       0  \n4       0  \n\n[5 rows x 29 columns]", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>id<\/th>\n      <th>cycle<\/th>\n      <th>setting1<\/th>\n      <th>setting2<\/th>\n      <th>setting3<\/th>\n      <th>s1<\/th>\n      <th>s2<\/th>\n      <th>s3<\/th>\n      <th>s4<\/th>\n      <th>s5<\/th>\n      <th>...<\/th>\n      <th>s15<\/th>\n      <th>s16<\/th>\n      <th>s17<\/th>\n      <th>s18<\/th>\n      <th>s19<\/th>\n      <th>s20<\/th>\n      <th>s21<\/th>\n      <th>RUL<\/th>\n      <th>label1<\/th>\n      <th>label2<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>1<\/td>\n      <td>1<\/td>\n      <td>-0.0007<\/td>\n      <td>-0.0004<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>641.82<\/td>\n      <td>1589.70<\/td>\n      <td>1400.60<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>8.4195<\/td>\n      <td>0.03<\/td>\n      <td>392<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>39.06<\/td>\n      <td>23.4190<\/td>\n      <td>191<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>1<\/td>\n      <td>2<\/td>\n      <td>0.0019<\/td>\n      <td>-0.0003<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.15<\/td>\n      <td>1591.82<\/td>\n      <td>1403.14<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>8.4318<\/td>\n      <td>0.03<\/td>\n      <td>392<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>39.00<\/td>\n      <td>23.4236<\/td>\n      <td>190<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>1<\/td>\n      <td>3<\/td>\n      <td>-0.0043<\/td>\n      <td>0.0003<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.35<\/td>\n      <td>1587.99<\/td>\n      <td>1404.20<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>8.4178<\/td>\n      <td>0.03<\/td>\n      <td>390<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>38.95<\/td>\n      <td>23.3442<\/td>\n      <td>189<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>1<\/td>\n      <td>4<\/td>\n      <td>0.0007<\/td>\n      <td>0.0000<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.35<\/td>\n      <td>1582.79<\/td>\n      <td>1401.87<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>8.3682<\/td>\n      <td>0.03<\/td>\n      <td>392<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>38.88<\/td>\n      <td>23.3739<\/td>\n      <td>188<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n    <\/tr>\n    <tr>\n      <th>4<\/th>\n      <td>1<\/td>\n      <td>5<\/td>\n      <td>-0.0019<\/td>\n      <td>-0.0002<\/td>\n      <td>100.0<\/td>\n      <td>518.67<\/td>\n      <td>642.37<\/td>\n      <td>1582.85<\/td>\n      <td>1406.22<\/td>\n      <td>14.62<\/td>\n      <td>...<\/td>\n      <td>8.4294<\/td>\n      <td>0.03<\/td>\n      <td>393<\/td>\n      <td>2388<\/td>\n      <td>100.0<\/td>\n      <td>38.90<\/td>\n      <td>23.4044<\/td>\n      <td>187<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n<p>5 rows \u00c3\u0097 29 columns<\/p>\n<\/div>"}, "metadata": {}}], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "In the [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) , cycle column is also used for training so we will also include the cycle column. Here, we normalize the columns in the training data.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "# MinMax normalization\ntrain_df['cycle_norm'] = train_df['cycle']\ncols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\nmin_max_scaler = preprocessing.MinMaxScaler()\nnorm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n                             columns=cols_normalize, \n                             index=train_df.index)\njoin_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\ntrain_df = join_df.reindex(columns = train_df.columns)\ntrain_df.head()", "outputs": [{"execution_count": 14, "output_type": "execute_result", "data": {"text/plain": "   id  cycle  setting1  setting2  setting3   s1        s2        s3        s4  \\\n0   1      1  0.459770  0.166667       0.0  0.0  0.183735  0.406802  0.309757   \n1   1      2  0.609195  0.250000       0.0  0.0  0.283133  0.453019  0.352633   \n2   1      3  0.252874  0.750000       0.0  0.0  0.343373  0.369523  0.370527   \n3   1      4  0.540230  0.500000       0.0  0.0  0.343373  0.256159  0.331195   \n4   1      5  0.390805  0.333333       0.0  0.0  0.349398  0.257467  0.404625   \n\n    s5     ...      s16       s17  s18  s19       s20       s21  RUL  label1  \\\n0  0.0     ...      0.0  0.333333  0.0  0.0  0.713178  0.724662  191       0   \n1  0.0     ...      0.0  0.333333  0.0  0.0  0.666667  0.731014  190       0   \n2  0.0     ...      0.0  0.166667  0.0  0.0  0.627907  0.621375  189       0   \n3  0.0     ...      0.0  0.333333  0.0  0.0  0.573643  0.662386  188       0   \n4  0.0     ...      0.0  0.416667  0.0  0.0  0.589147  0.704502  187       0   \n\n   label2  cycle_norm  \n0       0     0.00000  \n1       0     0.00277  \n2       0     0.00554  \n3       0     0.00831  \n4       0     0.01108  \n\n[5 rows x 30 columns]", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>id<\/th>\n      <th>cycle<\/th>\n      <th>setting1<\/th>\n      <th>setting2<\/th>\n      <th>setting3<\/th>\n      <th>s1<\/th>\n      <th>s2<\/th>\n      <th>s3<\/th>\n      <th>s4<\/th>\n      <th>s5<\/th>\n      <th>...<\/th>\n      <th>s16<\/th>\n      <th>s17<\/th>\n      <th>s18<\/th>\n      <th>s19<\/th>\n      <th>s20<\/th>\n      <th>s21<\/th>\n      <th>RUL<\/th>\n      <th>label1<\/th>\n      <th>label2<\/th>\n      <th>cycle_norm<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>1<\/td>\n      <td>1<\/td>\n      <td>0.459770<\/td>\n      <td>0.166667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.183735<\/td>\n      <td>0.406802<\/td>\n      <td>0.309757<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.0<\/td>\n      <td>0.333333<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.713178<\/td>\n      <td>0.724662<\/td>\n      <td>191<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n      <td>0.00000<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>1<\/td>\n      <td>2<\/td>\n      <td>0.609195<\/td>\n      <td>0.250000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.283133<\/td>\n      <td>0.453019<\/td>\n      <td>0.352633<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.0<\/td>\n      <td>0.333333<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.666667<\/td>\n      <td>0.731014<\/td>\n      <td>190<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n      <td>0.00277<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>1<\/td>\n      <td>3<\/td>\n      <td>0.252874<\/td>\n      <td>0.750000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.343373<\/td>\n      <td>0.369523<\/td>\n      <td>0.370527<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.0<\/td>\n      <td>0.166667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.627907<\/td>\n      <td>0.621375<\/td>\n      <td>189<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n      <td>0.00554<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>1<\/td>\n      <td>4<\/td>\n      <td>0.540230<\/td>\n      <td>0.500000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.343373<\/td>\n      <td>0.256159<\/td>\n      <td>0.331195<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.0<\/td>\n      <td>0.333333<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.573643<\/td>\n      <td>0.662386<\/td>\n      <td>188<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n      <td>0.00831<\/td>\n    <\/tr>\n    <tr>\n      <th>4<\/th>\n      <td>1<\/td>\n      <td>5<\/td>\n      <td>0.390805<\/td>\n      <td>0.333333<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.349398<\/td>\n      <td>0.257467<\/td>\n      <td>0.404625<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.0<\/td>\n      <td>0.416667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.589147<\/td>\n      <td>0.704502<\/td>\n      <td>187<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n      <td>0.01108<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n<p>5 rows \u00c3\u0097 30 columns<\/p>\n<\/div>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "Next, we prepare the test data. We first normalize the test data using the parameters from the MinMax normalization applied on the training data. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "test_df['cycle_norm'] = test_df['cycle']\nnorm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n                            columns=cols_normalize, \n                            index=test_df.index)\ntest_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\ntest_df = test_join_df.reindex(columns = test_df.columns)\ntest_df = test_df.reset_index(drop=True)\ntest_df.head()", "outputs": [{"execution_count": 15, "output_type": "execute_result", "data": {"text/plain": "   id  cycle  setting1  setting2  setting3   s1        s2        s3        s4  \\\n0   1      1  0.632184  0.750000       0.0  0.0  0.545181  0.310661  0.269413   \n1   1      2  0.344828  0.250000       0.0  0.0  0.150602  0.379551  0.222316   \n2   1      3  0.517241  0.583333       0.0  0.0  0.376506  0.346632  0.322248   \n3   1      4  0.741379  0.500000       0.0  0.0  0.370482  0.285154  0.408001   \n4   1      5  0.580460  0.500000       0.0  0.0  0.391566  0.352082  0.332039   \n\n    s5     ...           s13       s14       s15  s16       s17  s18  s19  \\\n0  0.0     ...      0.220588  0.132160  0.308965  0.0  0.333333  0.0  0.0   \n1  0.0     ...      0.264706  0.204768  0.213159  0.0  0.416667  0.0  0.0   \n2  0.0     ...      0.220588  0.155640  0.458638  0.0  0.416667  0.0  0.0   \n3  0.0     ...      0.250000  0.170090  0.257022  0.0  0.250000  0.0  0.0   \n4  0.0     ...      0.220588  0.152751  0.300885  0.0  0.166667  0.0  0.0   \n\n        s20       s21  cycle_norm  \n0  0.558140  0.661834     0.00000  \n1  0.682171  0.686827     0.00277  \n2  0.728682  0.721348     0.00554  \n3  0.666667  0.662110     0.00831  \n4  0.658915  0.716377     0.01108  \n\n[5 rows x 27 columns]", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>id<\/th>\n      <th>cycle<\/th>\n      <th>setting1<\/th>\n      <th>setting2<\/th>\n      <th>setting3<\/th>\n      <th>s1<\/th>\n      <th>s2<\/th>\n      <th>s3<\/th>\n      <th>s4<\/th>\n      <th>s5<\/th>\n      <th>...<\/th>\n      <th>s13<\/th>\n      <th>s14<\/th>\n      <th>s15<\/th>\n      <th>s16<\/th>\n      <th>s17<\/th>\n      <th>s18<\/th>\n      <th>s19<\/th>\n      <th>s20<\/th>\n      <th>s21<\/th>\n      <th>cycle_norm<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>1<\/td>\n      <td>1<\/td>\n      <td>0.632184<\/td>\n      <td>0.750000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.545181<\/td>\n      <td>0.310661<\/td>\n      <td>0.269413<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.220588<\/td>\n      <td>0.132160<\/td>\n      <td>0.308965<\/td>\n      <td>0.0<\/td>\n      <td>0.333333<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.558140<\/td>\n      <td>0.661834<\/td>\n      <td>0.00000<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>1<\/td>\n      <td>2<\/td>\n      <td>0.344828<\/td>\n      <td>0.250000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.150602<\/td>\n      <td>0.379551<\/td>\n      <td>0.222316<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.264706<\/td>\n      <td>0.204768<\/td>\n      <td>0.213159<\/td>\n      <td>0.0<\/td>\n      <td>0.416667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.682171<\/td>\n      <td>0.686827<\/td>\n      <td>0.00277<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>1<\/td>\n      <td>3<\/td>\n      <td>0.517241<\/td>\n      <td>0.583333<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.376506<\/td>\n      <td>0.346632<\/td>\n      <td>0.322248<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.220588<\/td>\n      <td>0.155640<\/td>\n      <td>0.458638<\/td>\n      <td>0.0<\/td>\n      <td>0.416667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.728682<\/td>\n      <td>0.721348<\/td>\n      <td>0.00554<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>1<\/td>\n      <td>4<\/td>\n      <td>0.741379<\/td>\n      <td>0.500000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.370482<\/td>\n      <td>0.285154<\/td>\n      <td>0.408001<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.250000<\/td>\n      <td>0.170090<\/td>\n      <td>0.257022<\/td>\n      <td>0.0<\/td>\n      <td>0.250000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.666667<\/td>\n      <td>0.662110<\/td>\n      <td>0.00831<\/td>\n    <\/tr>\n    <tr>\n      <th>4<\/th>\n      <td>1<\/td>\n      <td>5<\/td>\n      <td>0.580460<\/td>\n      <td>0.500000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.391566<\/td>\n      <td>0.352082<\/td>\n      <td>0.332039<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.220588<\/td>\n      <td>0.152751<\/td>\n      <td>0.300885<\/td>\n      <td>0.0<\/td>\n      <td>0.166667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.658915<\/td>\n      <td>0.716377<\/td>\n      <td>0.01108<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n<p>5 rows \u00c3\u0097 27 columns<\/p>\n<\/div>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "Next, we use the ground truth dataset to generate labels for the test data.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "# generate column max for test data\nrul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\nrul.columns = ['id', 'max']\ntruth_df.columns = ['more']\ntruth_df['id'] = truth_df.index + 1\ntruth_df['max'] = rul['max'] + truth_df['more']\ntruth_df.drop('more', axis=1, inplace=True)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 17, "cell_type": "code", "source": "# generate RUL for test data\ntest_df = test_df.merge(truth_df, on=['id'], how='left')\ntest_df['RUL'] = test_df['max'] - test_df['cycle']\ntest_df.drop('max', axis=1, inplace=True)\ntest_df.head()", "outputs": [{"execution_count": 17, "output_type": "execute_result", "data": {"text/plain": "   id  cycle  setting1  setting2  setting3   s1        s2        s3        s4  \\\n0   1      1  0.632184  0.750000       0.0  0.0  0.545181  0.310661  0.269413   \n1   1      2  0.344828  0.250000       0.0  0.0  0.150602  0.379551  0.222316   \n2   1      3  0.517241  0.583333       0.0  0.0  0.376506  0.346632  0.322248   \n3   1      4  0.741379  0.500000       0.0  0.0  0.370482  0.285154  0.408001   \n4   1      5  0.580460  0.500000       0.0  0.0  0.391566  0.352082  0.332039   \n\n    s5 ...        s14       s15  s16       s17  s18  s19       s20       s21  \\\n0  0.0 ...   0.132160  0.308965  0.0  0.333333  0.0  0.0  0.558140  0.661834   \n1  0.0 ...   0.204768  0.213159  0.0  0.416667  0.0  0.0  0.682171  0.686827   \n2  0.0 ...   0.155640  0.458638  0.0  0.416667  0.0  0.0  0.728682  0.721348   \n3  0.0 ...   0.170090  0.257022  0.0  0.250000  0.0  0.0  0.666667  0.662110   \n4  0.0 ...   0.152751  0.300885  0.0  0.166667  0.0  0.0  0.658915  0.716377   \n\n   cycle_norm  RUL  \n0     0.00000  142  \n1     0.00277  141  \n2     0.00554  140  \n3     0.00831  139  \n4     0.01108  138  \n\n[5 rows x 28 columns]", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>id<\/th>\n      <th>cycle<\/th>\n      <th>setting1<\/th>\n      <th>setting2<\/th>\n      <th>setting3<\/th>\n      <th>s1<\/th>\n      <th>s2<\/th>\n      <th>s3<\/th>\n      <th>s4<\/th>\n      <th>s5<\/th>\n      <th>...<\/th>\n      <th>s14<\/th>\n      <th>s15<\/th>\n      <th>s16<\/th>\n      <th>s17<\/th>\n      <th>s18<\/th>\n      <th>s19<\/th>\n      <th>s20<\/th>\n      <th>s21<\/th>\n      <th>cycle_norm<\/th>\n      <th>RUL<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>1<\/td>\n      <td>1<\/td>\n      <td>0.632184<\/td>\n      <td>0.750000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.545181<\/td>\n      <td>0.310661<\/td>\n      <td>0.269413<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.132160<\/td>\n      <td>0.308965<\/td>\n      <td>0.0<\/td>\n      <td>0.333333<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.558140<\/td>\n      <td>0.661834<\/td>\n      <td>0.00000<\/td>\n      <td>142<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>1<\/td>\n      <td>2<\/td>\n      <td>0.344828<\/td>\n      <td>0.250000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.150602<\/td>\n      <td>0.379551<\/td>\n      <td>0.222316<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.204768<\/td>\n      <td>0.213159<\/td>\n      <td>0.0<\/td>\n      <td>0.416667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.682171<\/td>\n      <td>0.686827<\/td>\n      <td>0.00277<\/td>\n      <td>141<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>1<\/td>\n      <td>3<\/td>\n      <td>0.517241<\/td>\n      <td>0.583333<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.376506<\/td>\n      <td>0.346632<\/td>\n      <td>0.322248<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.155640<\/td>\n      <td>0.458638<\/td>\n      <td>0.0<\/td>\n      <td>0.416667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.728682<\/td>\n      <td>0.721348<\/td>\n      <td>0.00554<\/td>\n      <td>140<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>1<\/td>\n      <td>4<\/td>\n      <td>0.741379<\/td>\n      <td>0.500000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.370482<\/td>\n      <td>0.285154<\/td>\n      <td>0.408001<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.170090<\/td>\n      <td>0.257022<\/td>\n      <td>0.0<\/td>\n      <td>0.250000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.666667<\/td>\n      <td>0.662110<\/td>\n      <td>0.00831<\/td>\n      <td>139<\/td>\n    <\/tr>\n    <tr>\n      <th>4<\/th>\n      <td>1<\/td>\n      <td>5<\/td>\n      <td>0.580460<\/td>\n      <td>0.500000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.391566<\/td>\n      <td>0.352082<\/td>\n      <td>0.332039<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.152751<\/td>\n      <td>0.300885<\/td>\n      <td>0.0<\/td>\n      <td>0.166667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.658915<\/td>\n      <td>0.716377<\/td>\n      <td>0.01108<\/td>\n      <td>138<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n<p>5 rows \u00c3\u0097 28 columns<\/p>\n<\/div>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "# generate label columns w0 and w1 for test data\ntest_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\ntest_df['label2'] = test_df['label1']\ntest_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\ntest_df.head()", "outputs": [{"execution_count": 18, "output_type": "execute_result", "data": {"text/plain": "   id  cycle  setting1  setting2  setting3   s1        s2        s3        s4  \\\n0   1      1  0.632184  0.750000       0.0  0.0  0.545181  0.310661  0.269413   \n1   1      2  0.344828  0.250000       0.0  0.0  0.150602  0.379551  0.222316   \n2   1      3  0.517241  0.583333       0.0  0.0  0.376506  0.346632  0.322248   \n3   1      4  0.741379  0.500000       0.0  0.0  0.370482  0.285154  0.408001   \n4   1      5  0.580460  0.500000       0.0  0.0  0.391566  0.352082  0.332039   \n\n    s5   ...    s16       s17  s18  s19       s20       s21  cycle_norm  RUL  \\\n0  0.0   ...    0.0  0.333333  0.0  0.0  0.558140  0.661834     0.00000  142   \n1  0.0   ...    0.0  0.416667  0.0  0.0  0.682171  0.686827     0.00277  141   \n2  0.0   ...    0.0  0.416667  0.0  0.0  0.728682  0.721348     0.00554  140   \n3  0.0   ...    0.0  0.250000  0.0  0.0  0.666667  0.662110     0.00831  139   \n4  0.0   ...    0.0  0.166667  0.0  0.0  0.658915  0.716377     0.01108  138   \n\n   label1  label2  \n0       0       0  \n1       0       0  \n2       0       0  \n3       0       0  \n4       0       0  \n\n[5 rows x 30 columns]", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>id<\/th>\n      <th>cycle<\/th>\n      <th>setting1<\/th>\n      <th>setting2<\/th>\n      <th>setting3<\/th>\n      <th>s1<\/th>\n      <th>s2<\/th>\n      <th>s3<\/th>\n      <th>s4<\/th>\n      <th>s5<\/th>\n      <th>...<\/th>\n      <th>s16<\/th>\n      <th>s17<\/th>\n      <th>s18<\/th>\n      <th>s19<\/th>\n      <th>s20<\/th>\n      <th>s21<\/th>\n      <th>cycle_norm<\/th>\n      <th>RUL<\/th>\n      <th>label1<\/th>\n      <th>label2<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>1<\/td>\n      <td>1<\/td>\n      <td>0.632184<\/td>\n      <td>0.750000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.545181<\/td>\n      <td>0.310661<\/td>\n      <td>0.269413<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.0<\/td>\n      <td>0.333333<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.558140<\/td>\n      <td>0.661834<\/td>\n      <td>0.00000<\/td>\n      <td>142<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>1<\/td>\n      <td>2<\/td>\n      <td>0.344828<\/td>\n      <td>0.250000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.150602<\/td>\n      <td>0.379551<\/td>\n      <td>0.222316<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.0<\/td>\n      <td>0.416667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.682171<\/td>\n      <td>0.686827<\/td>\n      <td>0.00277<\/td>\n      <td>141<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>1<\/td>\n      <td>3<\/td>\n      <td>0.517241<\/td>\n      <td>0.583333<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.376506<\/td>\n      <td>0.346632<\/td>\n      <td>0.322248<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.0<\/td>\n      <td>0.416667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.728682<\/td>\n      <td>0.721348<\/td>\n      <td>0.00554<\/td>\n      <td>140<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>1<\/td>\n      <td>4<\/td>\n      <td>0.741379<\/td>\n      <td>0.500000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.370482<\/td>\n      <td>0.285154<\/td>\n      <td>0.408001<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.0<\/td>\n      <td>0.250000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.666667<\/td>\n      <td>0.662110<\/td>\n      <td>0.00831<\/td>\n      <td>139<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n    <\/tr>\n    <tr>\n      <th>4<\/th>\n      <td>1<\/td>\n      <td>5<\/td>\n      <td>0.580460<\/td>\n      <td>0.500000<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.391566<\/td>\n      <td>0.352082<\/td>\n      <td>0.332039<\/td>\n      <td>0.0<\/td>\n      <td>...<\/td>\n      <td>0.0<\/td>\n      <td>0.166667<\/td>\n      <td>0.0<\/td>\n      <td>0.0<\/td>\n      <td>0.658915<\/td>\n      <td>0.716377<\/td>\n      <td>0.01108<\/td>\n      <td>138<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n<p>5 rows \u00c3\u0097 30 columns<\/p>\n<\/div>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "In the rest of the notebook, we train an LSTM network that we will compare to the results in [Predictive Maintenance Template Step 2B of 3](https://gallery.cortanaintelligence.com/Experiment/Predictive-Maintenance-Step-2B-of-3-train-and-evaluate-binary-classification-models-2) where a series of machine learning models are used to train and evaluate the binary classification model that uses column \"label1\" as the label.", "cell_type": "markdown", "metadata": {}}, {"source": "## Modelling\n\nThe traditional predictive maintenance machine learning models are based on feature engineering which is manual construction of right features using domain expertise and similar methods. This usually makes these models hard to reuse since feature engineering is specific to the problem scenario and the available data which varies from one business to the other. Perhaps the most attractive part of applying deep learning in the predictive maintenance domain is the fact that these networks can automatically extract the right features from the data, eliminating the need for manual feature engineering.\n\nWhen using LSTMs in the time-series domain, one important parameter to pick is the sequence length which is the window for LSTMs to look back. This may be viewed as similar to picking window_size = 5 cycles for calculating the rolling features in the [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) which are rolling mean and rolling standard deviation for 21 sensor values. The idea of using LSTMs is to let the model extract abstract features out of the sequence of sensor values in the window rather than engineering those manually. The expectation is that if there is a pattern in these sensor values within the window prior to failure, the pattern should be encoded by the LSTM.\n\nOne critical advantage of LSTMs is their ability to remember from long-term sequences (window sizes) which is hard to achieve by traditional feature engineering. For example, computing rolling averages over a window size of 50 cycles may lead to loss of information due to smoothing and abstracting of values over such a long period, istead, using all 50 values as input may provide better results. While feature engineering over large window sizes may not make sense, LSTMs are able to use larger window sizes and use all the information in the window as input. Below, we illustrate the approach.\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "# pick a large window size of 50 cycles\nsequence_length = 50", "outputs": [], "metadata": {"collapsed": true}}, {"source": "[Keras LSTM](https://keras.io/layers/recurrent/) layers expect an input in the shape of a numpy array of 3 dimensions (samples, time steps, features) where samples is the number of training sequences, time steps is the look back window or sequence length and features is the number of features of each sequence at each time step. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "# function to reshape features into (samples, time steps, features) \ndef gen_sequence(id_df, seq_length, seq_cols):\n    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n    we can use shorter ones \"\"\"\n    data_array = id_df[seq_cols].values\n    num_elements = data_array.shape[0]\n    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n        yield data_array[start:stop, :]", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 21, "cell_type": "code", "source": "# pick the feature columns \nsensor_cols = ['s' + str(i) for i in range(1,22)]\nsequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\nsequence_cols.extend(sensor_cols)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 22, "cell_type": "code", "source": "# generator for the sequences\nseq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n           for id in train_df['id'].unique())", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 23, "cell_type": "code", "source": "# generate sequences and convert to numpy array\nseq_array = np.concatenate(list(seq_gen))\nseq_array.shape", "outputs": [{"execution_count": 23, "output_type": "execute_result", "data": {"text/plain": "(15631, 50, 25)"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 24, "cell_type": "code", "source": "# function to generate labels\ndef gen_labels(id_df, seq_length, label):\n    data_array = id_df[label].values\n    num_elements = data_array.shape[0]\n    return data_array[seq_length:num_elements, :]", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 25, "cell_type": "code", "source": "# generate labels\nlabel_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label1']) \n             for id in train_df['id'].unique()]\nlabel_array = np.concatenate(label_gen)\nlabel_array.shape", "outputs": [{"execution_count": 25, "output_type": "execute_result", "data": {"text/plain": "(15631, 1)"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "## LSTM Network\nNext, we build a deep network. The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. Dropout is also applied after each LSTM layer to control overfitting. Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "# build the network\nnb_features = seq_array.shape[2]\nnb_out = label_array.shape[1]\n\nmodel = Sequential()\n\nmodel.add(LSTM(\n         input_shape=(sequence_length, nb_features),\n         units=100,\n         return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(\n          units=50,\n          return_sequences=False))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=nb_out, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 27, "cell_type": "code", "source": "# fit the network\nmodel.fit(seq_array, label_array, epochs=3, batch_size = 200, validation_split=0.05, verbose=2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Train on 14849 samples, validate on 782 samples\nEpoch 1/3\n21s - loss: 0.2533 - acc: 0.8921 - val_loss: 0.1122 - val_acc: 0.9527\nEpoch 2/3\n19s - loss: 0.0993 - acc: 0.9602 - val_loss: 0.0864 - val_acc: 0.9501\nEpoch 3/3\n20s - loss: 0.0815 - acc: 0.9674 - val_loss: 0.0394 - val_acc: 0.9859\n"}, {"execution_count": 27, "output_type": "execute_result", "data": {"text/plain": "<keras.callbacks.History at 0x7fa0c8370978>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 28, "cell_type": "code", "source": "print(model.summary())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (None, 50, 100)           50400     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 50, 100)           0         \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 50)                30200     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 51        \n=================================================================\nTotal params: 80,651\nTrainable params: 80,651\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"}], "metadata": {"collapsed": false}}, {"execution_count": 29, "cell_type": "code", "source": "# training metrics\nscores = model.evaluate(seq_array, label_array, verbose=2)\nprint('Accurracy: {}'.format(scores[1]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Accurracy: 0.9721706864564007\n"}], "metadata": {"collapsed": false}}, {"execution_count": 30, "cell_type": "code", "source": "# make predictions and compute confusion matrix\ny_pred = model.predict_classes(seq_array)\ny_true = label_array\nprint('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\ncm = confusion_matrix(y_true, y_pred)\ncm", "outputs": [{"output_type": "stream", "name": "stdout", "text": "15631/15631 [==============================] - 10s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nConfusion matrix\n- x-axis is true labels.\n- y-axis is predicted labels\n"}, {"execution_count": 30, "output_type": "execute_result", "data": {"text/plain": "array([[12226,   305],\n       [  130,  2970]])"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 31, "cell_type": "code", "source": "# compute precision and recall\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nprint( 'precision = ', precision, '\\n', 'recall = ', recall)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "precision =  0.906870229008 \n recall =  0.958064516129\n"}], "metadata": {"collapsed": false}}, {"source": "Next, we look at the performance on the test data. In the [Predictive Maintenance Template Step 1 of 3](https://gallery.cortanaintelligence.com/Experiment/Predictive-Maintenance-Step-1-of-3-data-preparation-and-feature-engineering-2), only the last cycle data for each engine id in the test data is kept for testing purposes. In order to compare the results to the template, we pick the last sequence for each id in the test data.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 32, "cell_type": "code", "source": "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n\nseq_array_test_last = np.asarray(seq_array_test_last)\nseq_array_test_last.shape", "outputs": [{"execution_count": 32, "output_type": "execute_result", "data": {"text/plain": "(93, 50, 25)"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 33, "cell_type": "code", "source": "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Similarly, we pick the labels.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 34, "cell_type": "code", "source": "label_array_test_last = test_df.groupby('id')['label1'].nth(-1)[y_mask].values\nlabel_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1)\nlabel_array_test_last.shape", "outputs": [{"execution_count": 34, "output_type": "execute_result", "data": {"text/plain": "(93, 1)"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 35, "cell_type": "code", "source": "print(seq_array_test_last.shape)\nprint(label_array_test_last.shape)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(93, 50, 25)\n(93, 1)\n"}], "metadata": {"collapsed": false}}, {"execution_count": 36, "cell_type": "code", "source": "# test metrics\nscores_test = model.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\nprint('Accurracy: {}'.format(scores_test[1]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Accurracy: 0.9784946307059257\n"}], "metadata": {"collapsed": false}}, {"execution_count": 37, "cell_type": "code", "source": "# make predictions and compute confusion matrix\ny_pred_test = model.predict_classes(seq_array_test_last)\ny_true_test = label_array_test_last\nprint('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\ncm = confusion_matrix(y_true_test, y_pred_test)\ncm", "outputs": [{"output_type": "stream", "name": "stdout", "text": "93/93 [==============================] - 0s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nConfusion matrix\n- x-axis is true labels.\n- y-axis is predicted labels\n"}, {"execution_count": 37, "output_type": "execute_result", "data": {"text/plain": "array([[66,  2],\n       [ 0, 25]])"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 38, "cell_type": "code", "source": "# compute precision and recall\nprecision_test = precision_score(y_true_test, y_pred_test)\nrecall_test = recall_score(y_true_test, y_pred_test)\nf1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\nprint( 'Precision: ', precision_test, '\\n', 'Recall: ', recall_test,'\\n', 'F1-score:', f1_test )", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Precision:  0.925925925926 \n Recall:  1.0 \n F1-score: 0.961538461538\n"}], "metadata": {"collapsed": false}}, {"execution_count": 39, "cell_type": "code", "source": "results_df = pd.DataFrame([[scores_test[1],precision_test,recall_test,f1_test],\n                          [0.94, 0.952381, 0.8, 0.869565]],\n                         columns = ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n                         index = ['LSTM',\n                                 'Template Best Model'])\nresults_df", "outputs": [{"execution_count": 39, "output_type": "execute_result", "data": {"text/plain": "                     Accuracy  Precision  Recall  F1-score\nLSTM                 0.978495   0.925926     1.0  0.961538\nTemplate Best Model  0.940000   0.952381     0.8  0.869565", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>Accuracy<\/th>\n      <th>Precision<\/th>\n      <th>Recall<\/th>\n      <th>F1-score<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>LSTM<\/th>\n      <td>0.978495<\/td>\n      <td>0.925926<\/td>\n      <td>1.0<\/td>\n      <td>0.961538<\/td>\n    <\/tr>\n    <tr>\n      <th>Template Best Model<\/th>\n      <td>0.940000<\/td>\n      <td>0.952381<\/td>\n      <td>0.8<\/td>\n      <td>0.869565<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n<\/div>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "Comparing the above test results to the predictive maintenance template, we see that the LSTM results are better than the template. It should be noted that the  data set used here is very small and deep learning models are known to perform superior with large datasets so for a more fair comparison larger datasets should be used.", "cell_type": "markdown", "metadata": {}}, {"source": "## Future Directions and Improvements\nThis tutorial is provided as starting point for using deep learning in predictive maintenance and depending on the data available ", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.1", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}